{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanKothari/SolarPredictor/blob/main/P1_Solar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0GRCLr4WU_6"
      },
      "source": [
        "# By: Sammy Korol and Aryan Kothari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGXpxUuvMeTd"
      },
      "source": [
        "#**Reading solar datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0yStkLqMdzD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ava = pd.read_pickle('ava_st1_ns4_23.pkl')\n",
        "comp = pd.read_pickle('comp_st1_ns4_23.pkl')\n",
        "seed = 100521223\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caQ-7L8wZCIw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# splitting input features (x) and response variable (y)\n",
        "target_column = 'energy'\n",
        "X = ava.drop(target_column, axis=1) # Selecting all columns except the last one\n",
        "y = ava[target_column]   # Selecting the last column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5miE0iMuRfoc"
      },
      "source": [
        "#**Exploratory Data Analysis (EDA**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXJWbTyWw6UM"
      },
      "outputs": [],
      "source": [
        "# displaying the first few rows of the dataset\n",
        "print(ava.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45SW4Dgbxo6f"
      },
      "outputs": [],
      "source": [
        "# print shape of X (input variables)\n",
        "print(X.shape) # 4380 instances and 300 input features\n",
        "\n",
        "# print shape of y (response variable)\n",
        "print(y.shape) # 4380 values of response variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2iP1Wua5gnF"
      },
      "outputs": [],
      "source": [
        "# Calculate mean, mode, median, and range of response variable\n",
        "mean_value = ava[\"energy\"].mean()\n",
        "mode_value = ava[\"energy\"].mode().iloc[0]\n",
        "median_value = ava[\"energy\"].median()\n",
        "range_value = ava[\"energy\"].max() - ava[\"energy\"].min()\n",
        "\n",
        "# Display the results\n",
        "print(f\"Mean: {mean_value} kJ\")\n",
        "print(f\"Mode: {mode_value} kJ\")\n",
        "print(f\"Median: {median_value} kJ\")\n",
        "print(f\"Range: {range_value} kJ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ_upjKbXgS5"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "missing_values = ava.isnull().sum().sum()\n",
        "print(f\"missing values: {missing_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5MWxK-SXt3A"
      },
      "outputs": [],
      "source": [
        "# identify constant columns\n",
        "constant_columns = ava.columns[ava.nunique() == 1]\n",
        "constant_value = ava[constant_columns[0]].iloc[0]\n",
        "print(f\"consant columns: {len(constant_columns)}\")\n",
        "print(f\"constant column name: {constant_columns[0]}\")\n",
        "print(f\"constant column value: {constant_value}\")\n",
        "\n",
        "#constant column is dropped\n",
        "ava.drop(columns=constant_columns, inplace=True)\n",
        "print(f\"Columns {constant_columns} have been dropped.\")\n",
        "\n",
        "#update X to reflect dropped column\n",
        "X = ava.drop(\"energy\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksiGILzQXQQ5",
        "outputId": "e24cfe9e-0870-490c-8a68-0f71251db17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "categorical columns: 19\n",
            "numerical columns: 282\n"
          ]
        }
      ],
      "source": [
        "# detect the presence of categorical variables\n",
        "categorical_columns = ava.select_dtypes(include=['category']).columns\n",
        "print(f\"categorical columns: {len(categorical_columns)}\")\n",
        "# detect the presence of numerical variables\n",
        "numerical_columns = ava.select_dtypes(include=['number']).columns\n",
        "print(f\"numerical columns: {len(numerical_columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TUX3X020vGy"
      },
      "source": [
        "## **EDA Analysis**\n",
        "\n",
        "**Categorical Variables:** there are 18 categorical variables detected, which will be dealt with as necessary depending on the model evaluation technique(s) below.\n",
        "\n",
        "**Missing Values:** there are no missing variables, hence no further action is required.\n",
        "\n",
        "**Constant Columns:** there is one constant column (\"apcp_sf3_3\") with value 1. Column is dropped to reduce redundancy and accuracy of model evaluations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJtzgnwSltLC"
      },
      "source": [
        "#**Method 1: Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3zOWu2ESMUF"
      },
      "source": [
        "## Training and evaluating a decision tree *without* hyperparameter tuning (holdout)\n",
        "\n",
        "First, we are going to use Holdout (train/test) for model evaluation without hyperparamter tuning. MSE and R^2 will be our metrics used to evaluate the performance of all models henceforth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-V4KUb3ltLS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Splitting data into testing (9 years) and training (3 years data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=3/12, random_state=100521223)\n",
        "\n",
        "# Training a decision tree (for regression)\n",
        "tree_reg = DecisionTreeRegressor(random_state=100521223)\n",
        "tree_reg.fit(X_train,y_train)\n",
        "\n",
        "# Obtain predictions on the test set\n",
        "y_pred_tree = tree_reg.predict(X_test)\n",
        "\n",
        "# Calculate RSME for the Decision Tree Regressor\n",
        "rmse_tree = metrics.mean_squared_error(y_test, y_pred_tree)\n",
        "print(f'MSE: {rmse_tree}')\n",
        "\n",
        "# Calculate R^2 for the Decision Tree Regressor\n",
        "r2_tree = r2_score(y_test, y_pred_tree)\n",
        "print(f'R^2 Score: {r2_tree}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9yVHYH1GNtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xly9ZljGVcW"
      },
      "source": [
        "## Hyperparameter tuning with CV\n",
        "Without hyperparameter tuning, the decision tree might be prone to overfitting the training data, and the model's performance on the validation set may not be optimal. Therefore, we use gridsearch & randomizedsearch to find optimal values for *max_depth* and *min_sample_split*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ClEZDAMUzi",
        "outputId": "a092adc1-60d1-4dd8-b651-decbaec62837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Mean Squared Error (With Hyperparameter Tuning): 13324816988333.227\n",
            "R^2 Score (With Hyperparameter Tuning): 0.7982468028318466\n"
          ]
        }
      ],
      "source": [
        "#Grid Search Hyperparamter Tuning\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'max_depth': list(range(2,16,2)),\n",
        "    'min_samples_split': list(range(2,16,2)),\n",
        "    }\n",
        "\n",
        "# Create decision tree regressor\n",
        "reg_tree_tuned = DecisionTreeRegressor(random_state=100521223)\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "grid_search = GridSearchCV(reg_tree_tuned, param_grid, cv=TimeSeriesSplit(n_splits=3), scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f'Best Hyperparameters: {best_params}')\n",
        "\n",
        "# Make predictions on the test set using the tuned model\n",
        "y_pred_tuned = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "mse_tuned = metrics.mean_squared_error(y_test, y_pred_tuned)\n",
        "print(f'Mean Squared Error (With Hyperparameter Tuning): {mse_tuned}')\n",
        "\n",
        "# Calculate R^2 for the Decision Tree Regressor\n",
        "r2_tree_tuned = r2_score(y_test, y_pred_tuned)\n",
        "print(f'R^2 Score (With Hyperparameter Tuning): {r2_tree_tuned}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomized Search Hyperparamter Tuning\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "from scipy.stats import uniform, expon\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "# Search space with integer uniform distributions\n",
        "param_grid = {'max_depth': sp_randint(2,16),\n",
        "              'min_samples_split': sp_randint(2,16)}\n",
        "\n",
        "budget = 20\n",
        "regr = RandomizedSearchCV(DecisionTreeRegressor(),\n",
        "                         param_grid,\n",
        "                         scoring='neg_mean_squared_error',\n",
        "                         cv=TimeSeriesSplit(n_splits=3),\n",
        "                         n_jobs=1, verbose=1,\n",
        "                         n_iter=budget\n",
        "                        )\n",
        "\n",
        "np.random.seed(100521223)\n",
        "regr.fit(X=X_train, y=y_train)\n",
        "\n",
        "best_params = regr.best_params_\n",
        "print(f'Best Hyperparameters: {best_params}')\n",
        "\n",
        "# Make predictions on the test set using the tuned model\n",
        "y_pred_tuned = regr.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "mse_tuned = metrics.mean_squared_error(y_test, y_pred_tuned)\n",
        "print(f'Mean Squared Error (With Hyperparameter Tuning): {mse_tuned}')\n",
        "\n",
        "# Calculate R^2 for the Decision Tree Regressor\n",
        "r2_tree_tuned = r2_score(y_test, y_pred_tuned)\n",
        "print(f'R^2 Score (With Hyperparameter Tuning): {r2_tree_tuned}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvxP4L2a6zNq",
        "outputId": "1bb80727-d4d7-4803-a04f-fb81f9f42382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'max_depth': 5, 'min_samples_split': 10}\n",
            "Mean Squared Error (With Hyperparameter Tuning): 12681659041934.867\n",
            "R^2 Score (With Hyperparameter Tuning): 0.8079849607430273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "As can be seen, the model which is trained and evaluated using hyperparameter tuning (max depth = 5, min samples split = 10) performs significantly better than the model trained and evaluated with default paramaters. This is made clear in the R^2 for each model, with the HPO model demonstrating an improvement of 0.08.\n",
        "\n",
        "In summary, model using randomized search HPO is the best performing regression tree mode."
      ],
      "metadata": {
        "id": "-cZpr3am40gI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDKaKtj-lug4"
      },
      "source": [
        "# **Method 2: KNN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Outliers in the Data:"
      ],
      "metadata": {
        "id": "6sMXWF35xoMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpnI_Hfgb2Q6",
        "outputId": "ee0bfd0a-fc0c-4abc-be9a-b3269c48787c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of data points with Z-scores above or equal to 7: 217\n"
          ]
        }
      ],
      "source": [
        "#Outlier Analysis:\n",
        "z_scores = np.abs((ava[numerical_columns] - ava[numerical_columns].mean()) / ava[numerical_columns].std())\n",
        "\n",
        "# Count the number of data points with Z-scores above threshold of 7\n",
        "outliers_count = (z_scores >= 7).sum(axis=1)\n",
        "\n",
        "# Total number of data points with Z-scores above the threshold\n",
        "total_outliers = (outliers_count > 0).sum()\n",
        "print(f\"Total number of data points with Z-scores above or equal to 7: {total_outliers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Model with standard k value (no hyper-parameter tuning)\n"
      ],
      "metadata": {
        "id": "RDNkofwfxrlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWey6GBNM_ki",
        "outputId": "c483ed49-e65f-4856-e54d-8b6d4f1aa853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 12381654019261.28\n",
            "R-squared (R^2): 0.7914587389793833\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#Identify extreme outliers (we decided on the outlier threshold to be below a z-score of 7, because 1/4 of the data points already had a z-score of 3)\n",
        "z_scores = np.abs((ava[numerical_columns] - ava[numerical_columns].mean()) / ava[numerical_columns].std())\n",
        "ava_no_outliers = ava[(z_scores < 7).all(axis=1)]\n",
        "\n",
        "# Features and target variable\n",
        "X = ava_no_outliers.drop('energy', axis=1)\n",
        "y = ava_no_outliers['energy']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "numerical_cols = X.select_dtypes(include=['number']).columns\n",
        "categorical_cols = X.select_dtypes(include=['category']).columns\n",
        "\n",
        "# Create transformers for numerical and categorical columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(drop='first'))  # Use drop='first' to avoid dummy variable trap\n",
        "])\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Build the KNN model with preprocessing\n",
        "knn_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor(n_neighbors=5))  # Not using Hyperparameter tuning\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R-squared (R^2): {r2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV tuning method (detecting lowest MSE):"
      ],
      "metadata": {
        "id": "NF29_ARxxEKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8OzjxwdccxN",
        "outputId": "67230e2e-130f-4573-d505-0cba81fcda14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Number of Neighbors for MSE: 10\n",
            "Mean Squared Error (MSE) for MSE model: 11484582098779.92\n",
            "R-squared (R^2) for MSE model: 0.8065679085000587\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for KNN\n",
        "param_grid = {'regressor__n_neighbors': [5, 10, 15, 20, 25]}  # Adjust the range as needed\n",
        "\n",
        "# Create a pipeline with preprocessing and KNN model\n",
        "knn_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor())\n",
        "])\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_n_neighbors_mse = grid_search.best_params_['regressor__n_neighbors']\n",
        "\n",
        "# Build the final KNN model with the best hyperparameters for MSE\n",
        "best_knn_model_mse = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor(n_neighbors=best_n_neighbors_mse))\n",
        "])\n",
        "\n",
        "# Train the model for MSE\n",
        "best_knn_model_mse.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for MSE\n",
        "y_pred_mse = best_knn_model_mse.predict(X_test)\n",
        "\n",
        "# Evaluate the model for MSE\n",
        "mse_mse = mean_squared_error(y_test, y_pred_mse)\n",
        "r2_mse = r2_score(y_test, y_pred_mse)\n",
        "\n",
        "print(f'Best Number of Neighbors for MSE: {best_n_neighbors_mse}')\n",
        "print(f'Mean Squared Error (MSE) for MSE model: {mse_mse}')\n",
        "print(f'R-squared (R^2) for MSE model: {r2_mse}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For-Loop tuning method (detecting lowest MSE)"
      ],
      "metadata": {
        "id": "eQWtgPclxOuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Adding Hyper-parameter tuning based on lowest MSE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for KNN\n",
        "param_grid = {'regressor__n_neighbors': [5, 10, 15, 20, 25]}  # Adjust the range as needed\n",
        "\n",
        "# Initialize variables to track the best hyperparameters\n",
        "best_mse = float('inf')  # Start with a very high value\n",
        "best_n_neighbors_mse = None\n",
        "\n",
        "# Iterate over each value of n_neighbors\n",
        "for n_neighbors in param_grid['regressor__n_neighbors']:\n",
        "    # Build the KNN model with the current hyperparameters\n",
        "    knn_model_temp = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', KNeighborsRegressor(n_neighbors=n_neighbors))\n",
        "    ])\n",
        "\n",
        "    # Train the model\n",
        "    knn_model_temp.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_temp = knn_model_temp.predict(X_test)\n",
        "\n",
        "    # Evaluate the model for MSE\n",
        "    mse_temp = mean_squared_error(y_test, y_pred_temp)\n",
        "\n",
        "    # Update the best hyperparameters if the current MSE is lower\n",
        "    if mse_temp < best_mse:\n",
        "        best_mse = mse_temp\n",
        "        best_n_neighbors_mse = n_neighbors\n",
        "\n",
        "# Build the final KNN model with the best hyperparameters for MSE\n",
        "best_knn_model_mse = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor(n_neighbors=best_n_neighbors_mse))\n",
        "])\n",
        "\n",
        "# Train the model for MSE\n",
        "best_knn_model_mse.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for MSE\n",
        "y_pred_mse = best_knn_model_mse.predict(X_test)\n",
        "\n",
        "# Evaluate the model for MSE\n",
        "mse_mse = mean_squared_error(y_test, y_pred_mse)\n",
        "r2_mse = r2_score(y_test, y_pred_mse)\n",
        "\n",
        "print(f'Best Number of Neighbors for MSE: {best_n_neighbors_mse}')\n",
        "print(f'Mean Squared Error (MSE) for MSE model: {mse_mse}')\n",
        "print(f'R-squared (R^2) for MSE model: {r2_mse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyIUGN8ExRpU",
        "outputId": "445fdf78-e844-41c0-e39b-6c13ec23e542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Number of Neighbors for MSE: 20\n",
            "Mean Squared Error (MSE) for MSE model: 11207266494203.098\n",
            "R-squared (R^2) for MSE model: 0.8112386694330632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV Tuning Method (detecting highest r^2)\n",
        "\n"
      ],
      "metadata": {
        "id": "l3MJJ4W7xT1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX0H0FUeccqL",
        "outputId": "a93d472b-11cf-47de-825d-c7d262af8755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Number of Neighbors for R-squared: 10\n",
            "Mean Squared Error (MSE) for R-squared model: 11484582098779.92\n",
            "R-squared (R^2) for R-squared model: 0.8065679085000587\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Define the parameter grid for KNN\n",
        "param_grid = {'regressor__n_neighbors': [5, 10, 15, 20, 25]}  # Adjust the range as needed\n",
        "\n",
        "# Create a pipeline with preprocessing and KNN model\n",
        "knn_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor())\n",
        "])\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search_r2 = GridSearchCV(knn_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "grid_search_r2.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_n_neighbors_r2 = grid_search_r2.best_params_['regressor__n_neighbors']\n",
        "\n",
        "# Build the final KNN model with the best hyperparameters for R-squared\n",
        "best_knn_model_r2 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor(n_neighbors=best_n_neighbors_r2))\n",
        "])\n",
        "\n",
        "# Train the model for R-squared\n",
        "best_knn_model_r2.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for R-squared\n",
        "y_pred_r2 = best_knn_model_r2.predict(X_test)\n",
        "\n",
        "# Evaluate the model for R-squared\n",
        "mse_r2 = mean_squared_error(y_test, y_pred_r2)\n",
        "r2_r2 = r2_score(y_test, y_pred_r2)\n",
        "\n",
        "print(f'Best Number of Neighbors for R-squared: {best_n_neighbors_r2}')\n",
        "print(f'Mean Squared Error (MSE) for R-squared model: {mse_r2}')\n",
        "print(f'R-squared (R^2) for R-squared model: {r2_r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Loop Tuning Method (highest r^2)"
      ],
      "metadata": {
        "id": "WibIzN_LxcXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding Hyper-parameter tuning based on highest R^2\n",
        "\n",
        "# Initialize variables to track the best hyperparameters\n",
        "best_r2 = -float('inf')  # Start with a very low value\n",
        "best_n_neighbors_r2 = None\n",
        "\n",
        "# Iterate over each value of n_neighbors\n",
        "for n_neighbors in param_grid['regressor__n_neighbors']:\n",
        "    # Build the KNN model with the current hyperparameters\n",
        "    knn_model_temp = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', KNeighborsRegressor(n_neighbors=n_neighbors))\n",
        "    ])\n",
        "\n",
        "    # Train the model\n",
        "    knn_model_temp.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_temp = knn_model_temp.predict(X_test)\n",
        "\n",
        "    # Evaluate the model for R-squared\n",
        "    r2_temp = r2_score(y_test, y_pred_temp)\n",
        "\n",
        "    # Update the best hyperparameters if the current R-squared is higher\n",
        "    if r2_temp > best_r2:\n",
        "        best_r2 = r2_temp\n",
        "        best_n_neighbors_r2 = n_neighbors\n",
        "\n",
        "# Build the final KNN model with the best hyperparameters for R-squared\n",
        "best_knn_model_r2 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', KNeighborsRegressor(n_neighbors=best_n_neighbors_r2))\n",
        "])\n",
        "\n",
        "# Train the model for R-squared\n",
        "best_knn_model_r2.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for R-squared\n",
        "y_pred_r2 = best_knn_model_r2.predict(X_test)\n",
        "\n",
        "# Evaluate the model for R-squared\n",
        "mse_r2 = mean_squared_error(y_test, y_pred_r2)\n",
        "r2_r2 = r2_score(y_test, y_pred_r2)\n",
        "\n",
        "print(f'Best Number of Neighbors for R-squared: {best_n_neighbors_r2}')\n",
        "print(f'Mean Squared Error (MSE) for R-squared model: {mse_r2}')\n",
        "print(f'R-squared (R^2) for R-squared model: {r2_r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy6iw6KBxgM9",
        "outputId": "036aa010-5832-4afb-dc7b-3ca555a5587b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Number of Neighbors for R-squared: 20\n",
            "Mean Squared Error (MSE) for R-squared model: 11207266494203.098\n",
            "R-squared (R^2) for R-squared model: 0.8112386694330632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**"
      ],
      "metadata": {
        "id": "uBaHHPFf6Oqo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BTPxHtPl50M"
      },
      "source": [
        "# **Method 3: SVMs**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2MWDROzb4EH"
      },
      "outputs": [],
      "source": [
        "#SVM Model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD6q2AQ1mA9D"
      },
      "source": [
        "# **Method 4: Random Forest & Extra Trees**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6eH72XhhFiB",
        "outputId": "ca207d9f-27f7-4b58-cdf8-6f2ad711d802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests Model:\n",
            "Mean Squared Error (MSE): 9326973023157.86\n",
            "R-squared (R^2): 0.8587788013170603\n"
          ]
        }
      ],
      "source": [
        "#Random Forest Model (no Hyper-parameter tuning):\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = ava.drop('energy', axis=1)\n",
        "y = ava['energy']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
        "\n",
        "# Create a pipeline with preprocessing and Random Forests model\n",
        "rf_model = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),  # You can choose whether or not to scale your features\n",
        "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=seed))  # This is without hyper-parameter tuning\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forests model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print('Random Forests Model:')\n",
        "print(f'Mean Squared Error (MSE): {mse_rf}')\n",
        "print(f'R-squared (R^2): {r2_rf}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RandomizedSearchCV for tuning on Random Forest (by lowest MSE)"
      ],
      "metadata": {
        "id": "hC7BKIwbTgZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBxB8LNPkTy-",
        "outputId": "31d84f94-3ebe-41f9-c966-8a531eafb2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests Model with Hyperparameter Tuning (RandomizedSearchCV):\n",
            "Best Hyperparameters: {'n_estimators': 267, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 110}\n",
            "Mean Squared Error (MSE): 9101328771949.059\n",
            "R-squared (R^2): 0.8621953172169672\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [int(x) for x in np.linspace(start=10, stop=300, num=10)],\n",
        "    'regressor__max_features': ['auto', 'sqrt'],\n",
        "    'regressor__max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
        "    'regressor__min_samples_split': [2, 5, 10],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV for hyperparameter tuning\n",
        "random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_grid, n_iter=2, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=seed)\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params_rf = random_search_rf.best_params_\n",
        "\n",
        "# Remove the 'regressor__' prefix from the parameter names\n",
        "best_params_rf = {key.replace('regressor__', ''): value for key, value in best_params_rf.items()}\n",
        "\n",
        "# Build the Random Forests model with the best hyperparameters\n",
        "best_rf_model = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', RandomForestRegressor(random_state=seed, **best_params_rf))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf_best = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Random Forests model with hyperparameter tuning\n",
        "mse_rf_best = mean_squared_error(y_test, y_pred_rf_best)\n",
        "r2_rf_best = r2_score(y_test, y_pred_rf_best)\n",
        "\n",
        "print('Random Forests Model with Hyperparameter Tuning (RandomizedSearchCV):')\n",
        "print(f'Best Hyperparameters: {best_params_rf}')\n",
        "print(f'Mean Squared Error (MSE): {mse_rf_best}')\n",
        "print(f'R-squared (R^2): {r2_rf_best}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Trees Model (without tuning)"
      ],
      "metadata": {
        "id": "8PtDOCSsT4EG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rxlllTWi9By",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c21faa1-4cfc-4034-ef80-fcc76ed33f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extra Trees Model:\n",
            "Mean Squared Error (MSE): 9276587062641.662\n",
            "R-squared (R^2): 0.8595417032492544\n"
          ]
        }
      ],
      "source": [
        "#Extra Trees Model (no Hyper-Parameter tuning):\n",
        "et_model = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),  # You can choose whether or not to scale your features\n",
        "    ('regressor', ExtraTreesRegressor(n_estimators=100, random_state=seed))  # This is without hyper-parameter tuning\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "et_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_et = et_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Extra Trees model\n",
        "mse_et = mean_squared_error(y_test, y_pred_et)\n",
        "r2_et = r2_score(y_test, y_pred_et)\n",
        "\n",
        "print('\\nExtra Trees Model:')\n",
        "print(f'Mean Squared Error (MSE): {mse_et}')\n",
        "print(f'R-squared (R^2): {r2_et}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RandomizedSearchCV for tuning on Extra Trees (by lowest MSE)"
      ],
      "metadata": {
        "id": "gHGGCo3cT_pq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzMMDVi6k82V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5aed7b-b3f3-4b1f-9c18-d22461408815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extra Trees Model with Hyperparameter Tuning (RandomizedSearchCV):\n",
            "Best Hyperparameters: {'n_estimators': 115, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Mean Squared Error (MSE): 9311107021114.018\n",
            "R-squared (R^2): 0.8590190310058752\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid for Extra Trees\n",
        "# Define the parameter grid for Extra Trees\n",
        "param_grid_et = {\n",
        "    'regressor__n_estimators': [int(x) for x in np.linspace(start=10, stop=200, num=10)],\n",
        "    'regressor__max_features': ['auto', 'sqrt'],\n",
        "    'regressor__max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
        "    'regressor__min_samples_split': [2, 5, 10],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV for hyperparameter tuning\n",
        "random_search_et = RandomizedSearchCV(et_model, param_distributions=param_grid_et, n_iter=2, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=seed)\n",
        "random_search_et.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params_et = random_search_et.best_params_\n",
        "\n",
        "# Remove the 'regressor__' prefix from the parameter names\n",
        "best_params_et = {key.replace('regressor__', ''): value for key, value in best_params_et.items()}\n",
        "\n",
        "# Build the Extra Trees model with the best hyperparameters\n",
        "best_et_model = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', ExtraTreesRegressor(random_state=seed, **best_params_et))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "best_et_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_et_best = best_et_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Extra Trees model with hyperparameter tuning\n",
        "mse_et_best = mean_squared_error(y_test, y_pred_et_best)\n",
        "r2_et_best = r2_score(y_test, y_pred_et_best)\n",
        "\n",
        "print('Extra Trees Model with Hyperparameter Tuning (RandomizedSearchCV):')\n",
        "print(f'Best Hyperparameters: {best_params_et}')\n",
        "print(f'Mean Squared Error (MSE): {mse_et_best}')\n",
        "print(f'R-squared (R^2): {r2_et_best}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**"
      ],
      "metadata": {
        "id": "PQ2bVHVR6TkL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawFZ5YdmGx3"
      },
      "source": [
        "# **Final Model: Estimation of Future Performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "372BBtoWmMSX"
      },
      "source": [
        "# **Saving Final Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n"
      ],
      "metadata": {
        "id": "3_eTw2pKhwkp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5miE0iMuRfoc",
        "X3zOWu2ESMUF",
        "zDKaKtj-lug4",
        "1BTPxHtPl50M"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}